{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f775bd",
   "metadata": {},
   "source": [
    "# VectorEmbeddingService Test Results\n",
    "\n",
    "This notebook tests the intent classification API routes against multiple prompts to evaluate performance.\n",
    "\n",
    "## API Routes Tested\n",
    "- `/api/intent/embedding` - Vector embedding-based classification\n",
    "- `/api/intent/slm` - Small Language Model classification\n",
    "- `/api/intent/hybrid` - Hybrid approach (vector + SLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617733c2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load test prompts and import required libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fadd504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.11/site-packages (25.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61dbd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21 test prompts from data/testPrompts.json\n",
      "Prompts structure: {intent, text}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Load test prompts from data/testPrompts.json\n",
    "# Each prompt has {intent, text} structure\n",
    "prompts_path = Path('data') / 'testPrompts.json'\n",
    "\n",
    "# If not found, try absolute path from current working directory\n",
    "if not prompts_path.exists():\n",
    "    import os\n",
    "    project_root = Path(os.getcwd())\n",
    "    prompts_path = project_root / 'data' / 'testPrompts.json'\n",
    "\n",
    "with open(prompts_path, 'r', encoding='utf-8') as f:\n",
    "    prompts_data = json.load(f)\n",
    "    test_prompts = prompts_data['testPrompts']\n",
    "\n",
    "print(f\"Loaded {len(test_prompts)} test prompts from {prompts_path}\")\n",
    "print(f\"Prompts structure: {{intent, text}}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06f2a2",
   "metadata": {},
   "source": [
    "## Test Functions\n",
    "\n",
    "Functions to test each API route.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64c279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding API test function defined\n"
     ]
    }
   ],
   "source": [
    "def test_embedding_api(prompts: List[Dict[str, Any]], api_url: str = 'http://localhost:3000/api/intent/embedding') -> List[Dict[str, Any]]:\n",
    "    \"\"\"Test the embedding API route.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt_obj in prompts:\n",
    "        text = prompt_obj.get('text', '')\n",
    "        expected_intent = prompt_obj.get('intent')\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                api_url,\n",
    "                json={'message': text},\n",
    "                headers={'Content-Type': 'application/json'},\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            actual_intent = data.get('intent', {}).get('intent', 'N/A')\n",
    "            is_correct = None\n",
    "            if expected_intent:\n",
    "                is_correct = actual_intent == expected_intent\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'actualIntent': actual_intent,\n",
    "                'isCorrect': is_correct,\n",
    "                'score': data.get('intent', {}).get('score', 0),\n",
    "                'candidates': data.get('candidates', []),\n",
    "                'route': 'embedding'\n",
    "            })\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'error': str(e),\n",
    "                'route': 'embedding'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'error': f'Unexpected error: {str(e)}',\n",
    "                'route': 'embedding'\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Embedding API test function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e81b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLM API test function defined\n"
     ]
    }
   ],
   "source": [
    "def test_slm_api(prompts: List[Dict[str, Any]], api_url: str = 'http://localhost:3000/api/intent/slm') -> List[Dict[str, Any]]:\n",
    "    \"\"\"Test the SLM API route.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt_obj in prompts:\n",
    "        text = prompt_obj.get('text', '')\n",
    "        expected_intent = prompt_obj.get('intent')\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                api_url,\n",
    "                json={'message': text},\n",
    "                headers={'Content-Type': 'application/json'},\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            classification = data.get('classification', {})\n",
    "            actual_intent = classification.get('intent', 'N/A')\n",
    "            is_correct = None\n",
    "            if expected_intent:\n",
    "                is_correct = actual_intent == expected_intent\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'actualIntent': actual_intent,\n",
    "                'isCorrect': is_correct,\n",
    "                'classification': classification,\n",
    "                'route': 'slm'\n",
    "            })\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'error': str(e),\n",
    "                'route': 'slm'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'error': f'Unexpected error: {str(e)}',\n",
    "                'route': 'slm'\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"SLM API test function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169c25fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid API test function defined\n"
     ]
    }
   ],
   "source": [
    "def test_hybrid_api(prompts: List[Dict[str, Any]], api_url: str = 'http://localhost:3000/api/intent/hybrid') -> List[Dict[str, Any]]:\n",
    "    \"\"\"Test the hybrid API route.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt_obj in prompts:\n",
    "        text = prompt_obj.get('text', '')\n",
    "        expected_intent = prompt_obj.get('intent')\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                api_url,\n",
    "                json={'message': text},\n",
    "                headers={'Content-Type': 'application/json'},\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            vector_matches = data.get('vectorMatches', [])\n",
    "            slm_result = data.get('slm', {})\n",
    "            \n",
    "            # Get intents from both approaches\n",
    "            vector_intent = vector_matches[0].get('intent', 'N/A') if vector_matches else 'N/A'\n",
    "            slm_intent = slm_result.get('intent', 'N/A')\n",
    "            \n",
    "            # Use SLM intent as primary (hybrid approach)\n",
    "            actual_intent = slm_intent if slm_intent != 'N/A' else vector_intent\n",
    "            is_correct = None\n",
    "            if expected_intent:\n",
    "                is_correct = actual_intent == expected_intent\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'actualIntent': actual_intent,\n",
    "                'vectorIntent': vector_intent,\n",
    "                'slmIntent': slm_intent,\n",
    "                'isCorrect': is_correct,\n",
    "                'vectorMatches': vector_matches,\n",
    "                'slmResult': slm_result,\n",
    "                'route': 'hybrid'\n",
    "            })\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'error': str(e),\n",
    "                'route': 'hybrid'\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'expectedIntent': expected_intent or 'N/A',\n",
    "                'error': f'Unexpected error: {str(e)}',\n",
    "                'route': 'hybrid'\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Hybrid API test function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8a38c",
   "metadata": {},
   "source": [
    "## Analysis Functions\n",
    "\n",
    "Functions to analyze and display test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0070157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results analysis function defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_results(results: List[Dict[str, Any]], route_name: str = '') -> Dict[str, Any]:\n",
    "    \"\"\"Analyze and display test results with accuracy metrics.\"\"\"\n",
    "    route_label = f\" ({route_name})\" if route_name else \"\"\n",
    "    \n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    correct_count = 0\n",
    "    incorrect_count = 0\n",
    "    intent_distribution = {}\n",
    "    accuracy_by_intent = {}\n",
    "    \n",
    "    for idx, result in enumerate(results, 1):\n",
    "        text = result.get('text', 'N/A')\n",
    "        \n",
    "        if 'error' in result:\n",
    "            error_count += 1\n",
    "        else:\n",
    "            expected = result.get('expectedIntent', 'N/A')\n",
    "            actual = result.get('actualIntent', 'N/A')\n",
    "            is_correct = result.get('isCorrect')\n",
    "            \n",
    "            if expected and expected != 'N/A':\n",
    "                if is_correct is True:\n",
    "                    correct_count += 1\n",
    "                elif is_correct is False:\n",
    "                    incorrect_count += 1\n",
    "            \n",
    "            # Track intent distribution\n",
    "            if actual != 'N/A':\n",
    "                intent_distribution[actual] = intent_distribution.get(actual, 0) + 1\n",
    "            \n",
    "            # Track accuracy by expected intent\n",
    "            if expected and expected != 'N/A' and is_correct is not None:\n",
    "                if expected not in accuracy_by_intent:\n",
    "                    accuracy_by_intent[expected] = {'correct': 0, 'total': 0}\n",
    "                accuracy_by_intent[expected]['total'] += 1\n",
    "                if is_correct:\n",
    "                    accuracy_by_intent[expected]['correct'] += 1\n",
    "            \n",
    "            success_count += 1\n",
    "    \n",
    "    print('\\n=== SUMMARY STATISTICS ===')\n",
    "    print(f'Total Prompts: {len(results)}')\n",
    "    print(f'Successful: {success_count}')\n",
    "    print(f'Errors: {error_count}')\n",
    "    \n",
    "    if correct_count + incorrect_count > 0:\n",
    "        accuracy = (correct_count / (correct_count + incorrect_count) * 100)\n",
    "        print(f'\\nAccuracy: {correct_count}/{correct_count + incorrect_count} ({accuracy:.2f}%)')\n",
    "        print(f'  Correct: {correct_count}')\n",
    "        print(f'  Incorrect: {incorrect_count}')\n",
    "    \n",
    "    print(f'\\nIntent Distribution (Predicted):')\n",
    "    for intent, count in sorted(intent_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f'  {intent}: {count}')\n",
    "    \n",
    "    if accuracy_by_intent:\n",
    "        print(f'\\nAccuracy by Intent:')\n",
    "        for intent, stats in sorted(accuracy_by_intent.items()):\n",
    "            acc = (stats['correct'] / stats['total'] * 100) if stats['total'] > 0 else 0\n",
    "            print(f'  {intent}: {stats[\"correct\"]}/{stats[\"total\"]} ({acc:.2f}%)')\n",
    "    \n",
    "    return {\n",
    "        'total': len(results),\n",
    "        'successful': success_count,\n",
    "        'errors': error_count,\n",
    "        'correct': correct_count,\n",
    "        'incorrect': incorrect_count,\n",
    "        'accuracy': (correct_count / (correct_count + incorrect_count) * 100) if (correct_count + incorrect_count) > 0 else None,\n",
    "        'intent_distribution': intent_distribution,\n",
    "        'accuracy_by_intent': accuracy_by_intent\n",
    "    }\n",
    "\n",
    "print(\"Results analysis function defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd2a26",
   "metadata": {},
   "source": [
    "## Run Tests\n",
    "\n",
    "Execute tests against all API routes. Make sure your Next.js server is running: `npm run dev`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665e671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Embedding API...\n",
      "\n",
      "=== TEST RESULTS SUMMARY (Embedding) ===\n",
      "\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total Prompts: 21\n",
      "Successful: 21\n",
      "Errors: 0\n",
      "\n",
      "Accuracy: 21/21 (100.00%)\n",
      "  Correct: 21\n",
      "  Incorrect: 0\n",
      "\n",
      "Intent Distribution (Predicted):\n",
      "  direct_product_search: 3\n",
      "  attribute_based_search: 3\n",
      "  problem_solving_search: 3\n",
      "  comparison_search: 3\n",
      "  project_based_search: 3\n",
      "  bulk_or_budget_search: 3\n",
      "  price_query: 3\n",
      "\n",
      "Accuracy by Intent:\n",
      "  attribute_based_search: 3/3 (100.00%)\n",
      "  bulk_or_budget_search: 3/3 (100.00%)\n",
      "  comparison_search: 3/3 (100.00%)\n",
      "  direct_product_search: 3/3 (100.00%)\n",
      "  price_query: 3/3 (100.00%)\n",
      "  problem_solving_search: 3/3 (100.00%)\n",
      "  project_based_search: 3/3 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# Test Embedding API\n",
    "print(\"Testing Embedding API...\")\n",
    "embedding_results = test_embedding_api(test_prompts)\n",
    "embedding_summary = analyze_results(embedding_results, 'Embedding')\n",
    "\n",
    "# Store results\n",
    "embedding_test_data = {\n",
    "    'results': embedding_results,\n",
    "    'summary': embedding_summary,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'route': 'embedding'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfa48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SLM API\n",
    "print(\"Testing SLM API...\")\n",
    "slm_results = test_slm_api(test_prompts)\n",
    "slm_summary = analyze_results(slm_results, 'SLM')\n",
    "\n",
    "# Store results\n",
    "slm_test_data = {\n",
    "    'results': slm_results,\n",
    "    'summary': slm_summary,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'route': 'slm'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hybrid API\n",
    "print(\"Testing Hybrid API...\")\n",
    "hybrid_results = test_hybrid_api(test_prompts)\n",
    "hybrid_summary = analyze_results(hybrid_results, 'Hybrid')\n",
    "\n",
    "# Store results\n",
    "hybrid_test_data = {\n",
    "    'results': hybrid_results,\n",
    "    'summary': hybrid_summary,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'route': 'hybrid'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdebf47",
   "metadata": {},
   "source": [
    "## Compare Results\n",
    "\n",
    "Compare performance across all three API routes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all routes\n",
    "print(\"\\n=== ROUTE COMPARISON ===\\n\")\n",
    "\n",
    "routes = [\n",
    "    ('Embedding', embedding_summary),\n",
    "    ('SLM', slm_summary),\n",
    "    ('Hybrid', hybrid_summary)\n",
    "]\n",
    "\n",
    "print(f\"{'Route':<15} {'Accuracy':<15} {'Correct':<10} {'Incorrect':<12} {'Errors':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for route_name, summary in routes:\n",
    "    accuracy = summary.get('accuracy')\n",
    "    accuracy_str = f\"{accuracy:.2f}%\" if accuracy is not None else \"N/A\"\n",
    "    correct = summary.get('correct', 0)\n",
    "    incorrect = summary.get('incorrect', 0)\n",
    "    errors = summary.get('errors', 0)\n",
    "    \n",
    "    print(f\"{route_name:<15} {accuracy_str:<15} {correct:<10} {incorrect:<12} {errors:<10}\")\n",
    "\n",
    "# Find best route\n",
    "best_route = max(routes, key=lambda x: x[1].get('accuracy', 0) if x[1].get('accuracy') is not None else 0)\n",
    "print(f\"\\nBest performing route: {best_route[0]} ({best_route[1].get('accuracy', 0):.2f}% accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c309f8",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export test results to JSON files for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca16a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(results: List[Dict[str, Any]], summary: Dict[str, Any], filename: str):\n",
    "    \"\"\"Export test results to JSON file.\"\"\"\n",
    "    output = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_prompts': len(results),\n",
    "        'results': results,\n",
    "        'summary': summary\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f' Results exported to {filename}')\n",
    "    return output\n",
    "\n",
    "# Export all results\n",
    "# Uncomment to export\n",
    "# export_results(embedding_results, embedding_summary, 'embedding_test_results.json')\n",
    "# export_results(slm_results, slm_summary, 'slm_test_results.json')\n",
    "# export_results(hybrid_results, hybrid_summary, 'hybrid_test_results.json')\n",
    "\n",
    "# Export combined comparison\n",
    "# combined_output = {\n",
    "#     'timestamp': datetime.now().isoformat(),\n",
    "#     'embedding': embedding_test_data,\n",
    "#     'slm': slm_test_data,\n",
    "#     'hybrid': hybrid_test_data\n",
    "# }\n",
    "# with open('all_routes_test_results.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(combined_output, f, indent=2, ensure_ascii=False)\n",
    "# print(' Combined results exported to all_routes_test_results.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0dc64",
   "metadata": {},
   "source": [
    "## Detailed Analysis with Pandas\n",
    "\n",
    "Display results in tabular format (optional, requires pandas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08269f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a more readable format using pandas (optional)\n",
    "try:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrame for embedding results\n",
    "    df_embedding = pd.DataFrame([\n",
    "        {\n",
    "            'text': r.get('text', 'N/A'),\n",
    "            'expected_intent': r.get('expectedIntent', 'N/A'),\n",
    "            'actual_intent': r.get('actualIntent', 'N/A'),\n",
    "            'is_correct': r.get('isCorrect'),\n",
    "            'score': r.get('score', 0),\n",
    "            'error': r.get('error', '')\n",
    "        }\n",
    "        for r in embedding_results\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n=== EMBEDDING RESULTS TABLE ===\")\n",
    "    print(df_embedding.to_string(index=False))\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for i, prompt_obj in enumerate(test_prompts):\n",
    "        text = prompt_obj.get('text', '')\n",
    "        expected = prompt_obj.get('intent', 'N/A')\n",
    "        \n",
    "        embedding_result = embedding_results[i] if i < len(embedding_results) else {}\n",
    "        slm_result = slm_results[i] if i < len(slm_results) else {}\n",
    "        hybrid_result = hybrid_results[i] if i < len(hybrid_results) else {}\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'text': text[:50] + '...' if len(text) > 50 else text,\n",
    "            'expected': expected,\n",
    "            'embedding': embedding_result.get('actualIntent', 'N/A'),\n",
    "            'slm': slm_result.get('actualIntent', 'N/A'),\n",
    "            'hybrid': hybrid_result.get('actualIntent', 'N/A'),\n",
    "            'embedding_correct': embedding_result.get('isCorrect'),\n",
    "            'slm_correct': slm_result.get('isCorrect'),\n",
    "            'hybrid_correct': hybrid_result.get('isCorrect')\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n=== ROUTE COMPARISON TABLE ===\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"pandas not available. Install with: pip install pandas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
